# 2.2 Redis Stream 实时管道 (Real-time Data Pipeline)

> **文档类型：** 架构文档 · 数据总线与契约篇
> **版本：** v1.0 · 2026-02-26
> **上游依赖：** [2.1 Protobuf 契约](./2.1_Protobuf契约.md) · [1.2 容器资源配额](../01_部署架构/1.2_容器资源配额.md)
> **下游文档：** [2.3 多链数据统一规范](./2.3_多链数据统一规范.md) · [3.1 策略 Agent 与水位线拦截](../03_核心执行引擎/3.1_策略Agent与水位线拦截.md)
>
> **合并说明：** 本文档由以下两份旧文档合并而来，原文档已废弃：
> - `数据基础设施_基于_Redis_Stream_的_Pub-Sub_流转拓扑.md`（提供 L1-L4 分层骨架、Topic 划分、Saga 跨链追踪）
> - `数据层_基于_Redis_Stream_的多_Agent_高频推送架构.md`（提供 Go 生产者实现、Python 消费者实现、性能指标、演进路线）

---

## 1. 设计原则与选型依据

### 1.1 为什么选 Redis Stream 而非 Redis Pub/Sub

传统 Redis Pub/Sub 是"发后即忘"模型：消息不持久化，消费者离线期间的消息永久丢失。对于承载资金决策的 DeFi 系统，这不可接受。

Redis Stream 解决了以下三个核心问题：

| 问题 | Redis Pub/Sub | Redis Stream |
|---|---|---|
| 消费者宕机重启后的消息恢复 | ❌ 消息丢失 | ✅ 从 PEL 恢复未 ACK 消息 |
| 50+ Agent 独立消费进度 | ❌ 所有订阅者共享同一流 | ✅ 每个 Consumer Group 独立 `Last_Delivered_ID` |
| 慢速 Agent 不影响快速 Agent | ❌ 共享订阅，互相阻塞 | ✅ Consumer Group 隔离，互不干扰 |

### 1.2 与 gRPC DataSentinel 的关系

`DataSentinel.SubscribeEvents`（见 §2.1）是面向 Python Agent 的 **gRPC 抽象接口**。其底层实现正是本文档描述的 Redis Stream 管道：Go 端将链上事件写入 Redis Stream，Python Agent 通过 `XREADGROUP` 消费；gRPC 接口将这一细节封装，使 Agent 代码无需直接操作 Redis 命令。

---

## 2. 数据流转四层模型（L1–L4）

系统数据流按职责分为四个物理层级，每层有明确的输入输出边界：

```
链上 RPC 节点
      │  newHeads / Logs (WebSocket)
      ▼
┌─────────────────────────────────────────┐
│  L1 · 原始接入层 (Raw Ingestion)         │
│  组件：Chain-Adapter (Go)               │
│  职责：订阅多 RPC 节点，解析 RLP 编码    │
│        打上摄取时间戳，不做任何业务处理   │
└──────────────────────┬──────────────────┘
                       │ 原始区块头 + 事件日志
                       ▼
┌─────────────────────────────────────────┐
│  L2 · 标准化与水位线层 (Normalization)   │
│  组件：Data-Processor (Go)              │
│  职责：转化为标准 Protobuf 格式          │
│        注入 last_sync_block（数据水位线）│
│        写入 Redis Stream 指定 Topic     │
└──────────────────────┬──────────────────┘
                       │ Protobuf 字节流 + 水位线标签
                       ▼
┌─────────────────────────────────────────┐
│  L3 · 消息总线层 (Redis Stream Bus)      │
│  组件：Redis Cluster                    │
│  职责：持久化消息中心                    │
│        支持 Consumer Group 并发读取     │
│        按 Topic 隔离不同类型的事件流     │
└──────────────────────┬──────────────────┘
                       │ XREADGROUP 消费
                       ▼
┌─────────────────────────────────────────┐
│  L4 · 消费与推理层 (Consumer Layer)      │
│  组件：50+ Python Agent 实例 (Zone A)   │
│  职责：消费链上事件，执行策略推理         │
│        本地水位线校验，过期消息跳过       │
│        向 Go 端提交 TradeRequest        │
└─────────────────────────────────────────┘
```

---

## 3. L1 — 原始接入层：Chain-Adapter（Go 生产者）

### 3.1 多源并行摄取

Chain-Adapter 通过多个 Goroutine 并发订阅多个 RPC 节点，确保数据摄取的冗余和速度：

```go
// chain_adapter.go（示意）
func (a *ChainAdapter) Start(ctx context.Context) {
    for _, rpcURL := range a.rpcNodes {
        go a.subscribeNode(ctx, rpcURL) // 每个 RPC 节点独立 Goroutine
    }
}

func (a *ChainAdapter) subscribeNode(ctx context.Context, rpcURL string) {
    client, _ := ethclient.DialContext(ctx, rpcURL)

    // 订阅新区块头
    headers := make(chan *types.Header)
    sub, _ := client.SubscribeNewHead(ctx, headers)

    for {
        select {
        case header := <-headers:
            // 打上摄取时间戳，传递给 L2
            a.rawCh <- &RawBlock{
                Header:        header,
                IngestTimeMs: time.Now().UnixMilli(),
            }
        case err := <-sub.Err():
            // 节点断线，触发重连逻辑（Sync Sentinel 会感知水位线停滞）
            log.Warn("RPC node disconnected", zap.String("url", rpcURL), zap.Error(err))
            return
        }
    }
}
```

**冗余保障：** 当主 RPC 节点断线时，Sync Sentinel 检测到数据水位线停滞，自动触发熔断（见 §3.1）。Chain-Adapter 的多节点订阅确保在单节点故障时，其他节点的数据流可以维持水位线推进。

---

## 4. L2 — 标准化与水位线层：Data-Processor（Go）

### 4.1 水位线注入

Data-Processor 是数据进入 L3 前的最后一道处理关卡，**强制为每条消息注入数据水位线**：

```go
// data_processor.go（示意）
func (p *DataProcessor) Process(raw *RawBlock) {
    // 1. 将原始数据转化为标准 Protobuf 格式
    event := &proto.ChainEvent{
        BlockNumber:  raw.Header.Number.Uint64(),
        BlockHash:    raw.Header.Hash().Hex(),
        ServerTimeMs: raw.IngestTimeMs,
    }

    // 2. 解析关联的事件日志
    for _, log := range raw.Logs {
        event.Logs = append(event.Logs, log.Data)
    }

    // 3. 强制注入水位线（last_sync_block）
    // 这是 Sync Sentinel 校验的依据，不可省略
    syncBlock := event.BlockNumber

    // 4. 序列化为 Protobuf 字节流，写入 Redis Stream
    payload, _ := proto.Marshal(event)
    p.redis.XAdd(ctx, &redis.XAddArgs{
        Stream: "stream:chain:events",
        MaxLen: p.cfg.RedisStreamMaxlen, // 对应配置键: redis_stream_maxlen（默认值见 README §0.6）
        Approx: true,                    // ~ 号：允许近似截断，性能更优
        Values: map[string]interface{}{
            "data":       payload,
            "sync_block": syncBlock,     // 水位线单独存为可索引字段
            "ingest_ts":  raw.IngestTimeMs,
        },
    })
}
```

> **`MAXLEN ~` 的含义：** `~` 表示近似截断（Approximate Trimming），Redis 不会在每次写入时精确裁剪到 `maxlen`，而是在内部 radix tree 节点满时批量裁剪，性能显著优于精确截断。对于 DeFi 时效性数据，近似截断完全可接受。`redis_stream_maxlen` 的默认值见 README §0.6，可通过配置文件覆盖。

---

## 5. L3 — 消息总线层：Redis Stream Topic 划分

### 5.1 Topic 定义

系统使用三个独立的 Stream，按数据性质和消费方隔离：

| Stream Topic | 生产者 | 消费者 | 内容 |
|---|---|---|---|
| `stream:chain:events` | Data-Processor (L2) | 全部 Python Agent | 实时区块头、协议事件日志、水位线标签 |
| `stream:internal:accounting` | FSM（交易 CONFIRMED 后） | 对账模块、跨链监控进程 | 双分录对账事件、跨链资金在途记录（见 §4.1） |
| `stream:agent:proposals` | Python Agent | 监控服务、影子模式审计 | Agent 发出的 TradeRequest 副本（只读，用于监控和 G7 影子对比） |

### 5.2 Consumer Group 设计

```
stream:chain:events
    ├── Consumer Group: Agent_001_Group  →  Python Agent 实例 001
    ├── Consumer Group: Agent_002_Group  →  Python Agent 实例 002
    ├── Consumer Group: Agent_003_Group  →  Python Agent 实例 003（同类策略横向扩展）
    │                                        ↑ 三个实例共享同一 Group ID
    │                                        Redis 自动负载均衡消息分发
    ├── Consumer Group: Archiver_Group   →  Archiver 服务（归档至 Parquet，见 §2.4）
    └── ...（最多支持 100+ Consumer Group）
```

**扩展规则：**
- **独立策略**：每种策略类型使用唯一 Group ID，各自维护独立的消费进度，互不影响
- **同类策略横向扩展**：多个相同策略实例共享同一 Group ID，Redis 自动将消息分发给组内空闲实例，实现负载均衡
- **新增 Agent 无需重启 Go 服务**：消费者加入 Group 只需客户端调用 `XGROUP CREATECONSUMER`，对 Zone B 零侵入

---

## 6. L4 — 消费与推理层：Python Agent（消费者）

### 6.1 异步消费循环实现

```python
# redis_consumer.py（示意）
import asyncio
import redis.asyncio as aioredis
from decimal import Decimal

class RedisStreamConsumer:
    def __init__(self, agent_id: str, group_id: str, redis_client):
        self.agent_id  = agent_id
        self.group_id  = group_id
        self.redis     = redis_client
        self.last_seen_block = 0  # 本地维护的水位线

    async def consume_loop(self):
        """主消费循环：阻塞读取，100ms 超时释放 CPU"""
        while True:
            messages = await self.redis.xreadgroup(
                groupname  = self.group_id,
                consumername = self.agent_id,
                streams    = {"stream:chain:events": ">"},  # ">" 表示只读未投递消息
                count      = 10,    # 每批最多读取 10 条，控制单次处理延迟
                block      = 100,   # 阻塞等待 100ms，无新数据时释放 CPU
            )

            if not messages:
                continue

            for stream, entries in messages:
                for entry_id, fields in entries:
                    await self._handle_entry(entry_id, fields)

    async def _handle_entry(self, entry_id: str, fields: dict):
        """处理单条消息，含本地水位线校验"""
        sync_block = int(fields[b"sync_block"])
        ingest_ts  = int(fields[b"ingest_ts"])

        # ── 本地水位线熔断（Stale Data Kill-switch）────────────────────────
        # 校验 1：区块高度是否落后
        # 注意：这是 Agent 本地的辅助校验，最终的硬拦截由 Go 端 Sync Sentinel 执行
        if self.last_seen_block > 0 and sync_block < self.last_seen_block - 2:
            # 消息来自过旧的区块，仅更新本地状态，禁止触发交易决策
            self._update_local_state(fields)
            await self.redis.xack("stream:chain:events", self.group_id, entry_id)
            return

        # 校验 2：摄取延迟是否过高（消息在 Redis 中积压）
        now_ms   = int(asyncio.get_event_loop().time() * 1000)
        delay_ms = now_ms - ingest_ts
        if delay_ms > 500:  # 超过 500ms 视为积压消息，跳过交易决策
            self._update_local_state(fields)
            await self.redis.xack("stream:chain:events", self.group_id, entry_id)
            return
        # ──────────────────────────────────────────────────────────────────

        # 更新本地水位线
        self.last_seen_block = sync_block

        # 执行策略推理（由子类实现）
        await self.on_new_block(sync_block, fields)

        # 策略处理完成，发送 ACK（表示此消息已被有效消费）
        # 只有在 ExecuteTrade gRPC 调用成功返回后才 ACK，确保可靠性
        await self.redis.xack("stream:chain:events", self.group_id, entry_id)

    async def on_new_block(self, sync_block: int, fields: dict):
        """由具体策略子类实现推理逻辑"""
        raise NotImplementedError
```

### 6.2 XPENDING 监控：慢速 Agent 预警

通过监控 `XPENDING` 列表，运维系统可实时感知哪些 Agent 处理速度跟不上区块产生速度：

```python
# 运维监控脚本（示意）
async def check_pending_backlog(redis_client):
    pending = await redis_client.xpending(
        "stream:chain:events",
        "Agent_001_Group"
    )
    # pending["pending"] 为待 ACK 的消息数
    if pending["pending"] > 100:
        # 触发 §6.3 告警：Agent 处理积压过大
        alert("HIGH_PENDING_BACKLOG", agent="Agent_001", count=pending["pending"])
```

---

## 7. 跨链资金追踪：Saga 模式（`stream:internal:accounting`）

当策略执行跨链操作（如从 Ethereum 向 Arbitrum 转移资金）时，资金存在"在途"中间状态，双链账本均需记录：

```
跨链操作触发时：

Go FSM（Ethereum 链）
    └── 向 stream:internal:accounting 写入：
        { type: "IN_TRANSIT", from_chain: 1, to_chain: 42161,
          amount: "1000000000", bridge_tx_hash: "0x...",
          deadline_ms: now + 30min }

独立监控进程订阅 stream:internal:accounting：
    ├── 若 30 分钟内收到 Arbitrum 的 CONFIRMED 事件 → 标记在途资金到账，完成对账
    └── 若 30 分钟超时未确认 → 触发 Saga 回滚：
            · 记录跨链失败告警
            · 将资金状态标回 "AVAILABLE"
            · 通知管理员介入检查 Bridge 状态
```

> **为什么用 30 分钟超时？** 主流跨链桥（Hop、Across、Stargate）的最终确认时间通常在 5–20 分钟内。30 分钟是保守阈值，覆盖网络拥堵场景。此超时值为系统内置默认值，如有需要可在配置中调整。

---

## 8. 物理性能指标

| 指标 | 目标值 | 说明 |
|---|---|---|
| **L2 → L4 分发延迟** | < 10ms | Redis 内存操作延迟，确保 AI 推理有足够响应窗口 |
| **端到端 P99 延迟** | < 500ms | 从链上事件到 FSM 推进至 SIMULATED 状态的全链路延迟 |
| **并发消费者承载** | 100+ Consumer Group | 预留当前 50+ Agent 规模 2 倍扩展空间 |
| **消息丢失率** | 0% | Redis Stream + AOF 持久化保证，Agent 崩溃从 PEL 恢复 |
| **数据水位线漂移** | < 1 个区块 | L2 注入水位线后，L4 消费延迟不超过一个出块周期 |

---

## 9. 两份旧文档的差异处理说明

| 差异项 | 旧文档 A（Pub-Sub 拓扑） | 旧文档 B（多 Agent 推送） | 裁决结果 |
|---|---|---|---|
| 文档骨架 | L1-L4 分层模型（结构更清晰） | 生产者/消费者视角 | **以 A 的 L1-L4 为骨架，B 的实现细节填入各层** |
| Go 生产者细节 | 仅概述 | Goroutine 多源摄取、XADD MAXLEN 参数 | **采用 B，填入 §3/§4** |
| Python 消费者实现 | 仅概述 | asyncio、XREADGROUP、BLOCK=100ms | **采用 B，填入 §6** |
| 性能指标表 | 无 | 有（4 项指标） | **采用 B，补充至 §8** |
| 架构演进路线 | 无 | NATS/Kafka 迁移方案 | **采用 B，补充至 §10** |
| Saga 跨链追踪 | 有（30 分钟超时） | 无 | **采用 A，独立成 §7** |
| 错误模块引用 | "模块 4.1"、"模块 4.2" | — | **统一修正为 §3.1 Sync Sentinel、§4.1** |

---

## 10. 架构演进路线

Redis Stream 在当前 50+ Agent 规模下完全满足需求。若未来扩展至 200+ 策略、资产管理规模超过千万美金，建议评估以下迁移路径：

| 触发条件 | 建议方案 | 迁移成本 |
|---|---|---|
| Agent 数量 > 200，Redis 内存成为瓶颈 | 迁移至 **NATS JetStream**（轻量，延迟更低） | 低：仅需替换 Data-Processor 的写入驱动 |
| 需要更细粒度的分区和回放能力 | 迁移至 **Apache Kafka** | 中：需引入 Kafka 运维体系，但 Python/Go 业务代码无需改动 |

**迁移零侵入保证：** Go 端的 Data-Processor 对消息总线的写入操作已通过接口抽象，更换底层 Broker 只需实现新的驱动接口，Python Agent 的消费逻辑和策略代码完全不受影响。

---

*上一篇：[2.1 Protobuf 契约](./2.1_Protobuf契约.md) | 下一篇：[2.3 多链数据统一规范](./2.3_多链数据统一规范.md)*
