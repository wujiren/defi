# 2.4 历史数据归档 (Historical Data Archiving)

> **文档类型：** 架构文档 · 数据总线与契约篇
> **版本：** v1.0 · 2026-02-26
> **上游依赖：** [2.2 Redis Stream 实时管道](./2.2_Redis_Stream实时管道.md) · [2.3 多链数据统一规范](./2.3_多链数据统一规范.md)
> **下游文档：** [7.1 回测引擎与数据接口](../07_验证与回测/7.1_回测引擎与数据接口.md)
>
> **PRD 主权声明：** 数据精度要求、存储规范、可复现性要求定义在 PRD《数据基础设施规范》第四节，本文档描述其架构实现。
>
> **职责边界：** 本文档只覆盖归档侧（写入 Parquet）。读取侧（`IDataProvider` 接口、`BacktestProvider` 实现）见 §7.1。

---

## 1. 设计背景：为什么需要归档管道

Redis Stream 作为实时消息总线（见 §2.2），其 `redis_stream_maxlen` 配置控制保留的消息窗口（默认值见 README §0.6），对应约 **3–10 分钟**的链上数据。这是刻意的设计——Redis 内存不适合存储全量历史数据。

归档管道解决的核心问题：

| 场景 | Redis Stream 的局限 | 归档管道的解决方式 |
|---|---|---|
| G5 历史回测（需要数月数据） | 内存有限，最多保留分钟级数据 | Parquet 冷存储，永久保留全量 Tick 数据 |
| G6 蒙特卡洛模拟（需要高频随机采样） | 无法高效随机访问历史区间 | 列式存储支持谓词下推，按区块范围快速定位 |
| 审计与可复现性 | 数据滚动删除，无法追溯 | 每个回测任务锁定数据集哈希，可精确复现 |
| 数据完整性验证 | 无内置校验机制 | 每日区块哈希校验，检测篡改或损坏 |

---

## 2. 数据分层架构（热-温-冷）

```
┌────────────────────────────────────────────────────────────────┐
│  热数据层（Hot）· Redis Stream                                  │
│  · 保留：由 redis_stream_maxlen 控制（约 3–10 分钟）             │
│  · 存储：内存（Redis AOF 持久化）                               │
│  · 用途：实盘 Agent 实时消费、水位线校验                         │
│  · 配置键：redis_stream_maxlen（默认值见 README §0.6）           │
└──────────────────────────┬─────────────────────────────────────┘
                           │ Archiver 消费（XREADGROUP）
                           │ 批量写入（由 archiver_batch_size 和 archiver_flush_interval_sec 控制）
                           ▼
┌────────────────────────────────────────────────────────────────┐
│  温数据层（Warm）· 本地 SSD / NAS                               │
│  · 保留：近 90 天 Parquet 文件                                  │
│  · 存储：高速 SSD，支持快速随机访问                              │
│  · 用途：近期回测（G5/G6）、影子模式数据对比                     │
└──────────────────────────┬─────────────────────────────────────┘
                           │ 90 天后自动迁移
                           ▼
┌────────────────────────────────────────────────────────────────┐
│  冷数据层（Cold）· S3 / MinIO 对象存储                           │
│  · 保留：永久（全量历史，不删除）                                │
│  · 存储：HDD 或对象存储，成本优先                               │
│  · 用途：长期回测、审计复现、监管合规存档                        │
└────────────────────────────────────────────────────────────────┘
                           │
                           ▼
┌────────────────────────────────────────────────────────────────┐
│  索引层（Index）· ClickHouse                                    │
│  · 内容：Parquet 文件元数据（路径、区块范围、哈希、大小）         │
│  · 用途：回测引擎快速定位目标数据文件（无需全量扫描 S3）          │
└────────────────────────────────────────────────────────────────┘
```

---

## 3. Go Archiver 服务

Archiver 是部署在 Zone B 的轻量级后台服务，作为 Redis Stream 的独立 Consumer Group，**只读不写**，职责单一：将热数据持续沉淀为 Parquet 冷存储。

### 3.1 核心逻辑

```go
// archiver.go
type Archiver struct {
    redis      *redis.Client
    s3         *s3.Client
    clickhouse *clickhouse.Conn
    cfg        ArchiverConfig
    buf        []*proto.ChainEvent // 内存缓冲区
}

type ArchiverConfig struct {
    BatchSize     int           // 对应配置键: archiver_batch_size（默认值见 README §0.6）
    FlushInterval time.Duration // 对应配置键: archiver_flush_interval_sec（默认值见 README §0.6）
    S3Bucket      string        // 目标 S3 桶名
    WarmDataPath  string        // 温数据本地路径（近 90 天）
}

func (a *Archiver) Run(ctx context.Context) {
    ticker := time.NewTicker(a.cfg.FlushInterval)
    defer ticker.Stop()

    for {
        // 从 Redis Stream 批量读取（非阻塞，立即返回已有消息）
        msgs, err := a.redis.XReadGroup(ctx, &redis.XReadGroupArgs{
            Group:    "Archiver_Group",
            Consumer: "archiver-0",
            Streams:  []string{"stream:chain:events", ">"},
            Count:    int64(a.cfg.BatchSize),
            Block:    0, // 非阻塞
        }).Result()

        if err == nil && len(msgs) > 0 {
            for _, msg := range msgs[0].Messages {
                event := a.unmarshal(msg.Values)
                a.buf = append(a.buf, event)
            }
        }

        // 触发刷写：批量满 OR 定时到达（两者取先）
        shouldFlush := len(a.buf) >= a.cfg.BatchSize
        select {
        case <-ticker.C:
            shouldFlush = true
        default:
        }

        if shouldFlush && len(a.buf) > 0 {
            if err := a.flush(ctx); err != nil {
                // 写入失败：保留缓冲区内容，下次重试；记录告警
                log.Error("archiver flush failed", zap.Error(err))
            }
        }
    }
}
```

### 3.2 Parquet 写入与分区策略

```go
func (a *Archiver) flush(ctx context.Context) error {
    if len(a.buf) == 0 {
        return nil
    }

    // ── 按链 + 协议 + 日期三级分区 ──────────────────────────────────────
    // S3 路径格式：
    //   s3://{bucket}/chain_id={chainID}/protocol_id={protocolID}/date={YYYY-MM-DD}/
    //       anchor_eth_block={startBlock}-{endBlock}/partition_{N}.parquet
    //
    // anchor_eth_block 子目录：使得回测引擎可按区块范围精确定位文件，无需全量扫描
    firstEvent := a.buf[0]
    lastEvent  := a.buf[len(a.buf)-1]
    s3Path := fmt.Sprintf(
        "s3://%s/chain_id=%d/protocol_id=%s/date=%s/anchor_eth_block=%d-%d/partition_%d.parquet",
        a.cfg.S3Bucket,
        firstEvent.SourceChainID,
        firstEvent.ProtocolID,
        firstEvent.AnchorTimestamp.Format("2006-01-02"),
        firstEvent.AnchorEthBlock,
        lastEvent.AnchorEthBlock,
        time.Now().UnixNano(),
    )

    // ── 构建 Parquet 列式文件（使用 Apache Arrow Go 库）─────────────────
    parquetBytes, dataHash, err := a.buildParquet(a.buf)
    if err != nil {
        return fmt.Errorf("build parquet: %w", err)
    }

    // ── 上传至 S3 ────────────────────────────────────────────────────────
    if err := a.uploadToS3(ctx, s3Path, parquetBytes); err != nil {
        return fmt.Errorf("upload to s3: %w", err)
    }

    // ── 写入 ClickHouse 索引层 ───────────────────────────────────────────
    if err := a.indexInClickHouse(ctx, s3Path, firstEvent, lastEvent, dataHash, len(a.buf)); err != nil {
        // ClickHouse 写入失败不阻塞主流程，但需告警（数据已在 S3，索引可异步补写）
        log.Warn("clickhouse index write failed", zap.String("path", s3Path), zap.Error(err))
    }

    // ── ACK 并清空缓冲区 ────────────────────────────────────────────────
    a.ackAll(ctx)
    a.buf = a.buf[:0]
    return nil
}
```

---

## 4. Parquet 文件 Schema

每个 Parquet 文件对应一批 Tick 数据，Schema 设计以**回测引擎的查询模式**为优先（按区块范围过滤 + 按协议聚合）：

```
Parquet Schema: chain_events
├── anchor_eth_block    UINT64      非空  Ethereum 锚点区块（分区键，谓词下推）
├── source_chain_id     INT32       非空  来源链 Chain ID
├── source_block        UINT64      非空  来源链本地区块高度
├── protocol_id         STRING      非空  协议标识，如 "UNISWAP_V3_ETH"
├── event_type          STRING      非空  事件类型：Swap / Mint / Burn / Collect
├── pool_address        STRING      非空  池子合约地址（校验和格式）
├── token_in_canonical  STRING      可空  输入代币 canonical_id（见 §2.3）
├── token_out_canonical STRING      可空  输出代币 canonical_id
├── amount_in_raw       STRING      可空  输入数量（原始整数字符串，防精度丢失）
├── amount_out_raw      STRING      可空  输出数量（原始整数字符串）
├── price_usdc          DECIMAL(20,6) 可空 执行价（USDC 计价，见 §2.3 §4）
├── liquidity_after     STRING      可空  事件后池子流动性（原始整数）
├── gas_used            UINT64      可空  该笔交易的 gas 消耗
├── gas_price_wei       STRING      可空  effective gas price（原始整数）
├── gas_cost_usd        DECIMAL(20,6) 可空 Gas 成本（USD 计价，见 §2.3 §6）
├── tx_hash             STRING      非空  交易哈希
├── log_index           INT32       非空  事件在交易收据中的日志索引
├── ingest_timestamp_ms INT64       非空  Go 端摄取时间戳（毫秒）
└── anchor_timestamp_ms INT64       非空  ETH 锚点区块时间戳（毫秒）
```

**列式存储优化说明：**
- `anchor_eth_block` 作为 Parquet Row Group 的排序键，回测引擎按区块范围查询时直接跳过不相关的 Row Group（谓词下推）
- 所有金额字段使用 `STRING` 而非数值类型，与 Protobuf 和数据库保持一致，避免精度丢失
- `price_usdc` 和 `gas_cost_usd` 在归档时预计算并写入，回测引擎读取时无需二次换算

---

## 5. ClickHouse 索引层 Schema

索引层只存储元数据，不存储 Tick 数据本身，供回测引擎快速定位目标 Parquet 文件。

**设计原则：主表纯追加，违规状态独立存储。** `parquet_file_index` 作为 append-only 的事实表，写入后永不修改（`MergeTree` 最适合此场景）。完整性违规状态写入独立的轻量表 `file_integrity_violations`，避免在主表上触发 ClickHouse 的异步 Mutation 操作。

```sql
-- 主索引表（append-only，MergeTree 最佳适配场景）
CREATE TABLE parquet_file_index
(
    file_path           String,       -- 完整 S3 路径（或本地温数据路径）
    source_chain_id     Int32,
    protocol_id         String,
    date                Date,         -- 文件覆盖的日期（分区键）
    start_eth_block     UInt64,       -- 文件内最小 anchor_eth_block
    end_eth_block       UInt64,       -- 文件内最大 anchor_eth_block
    row_count           UInt32,       -- Tick 条数
    file_size_bytes     UInt64,       -- 文件大小（字节）
    data_sha256         String,       -- 写入时的 SHA-256 哈希（用于完整性校验和可复现性）
    created_at          DateTime      -- 归档写入时间
)
ENGINE = MergeTree()
PARTITION BY (source_chain_id, toYYYYMM(date))
ORDER BY (source_chain_id, protocol_id, start_eth_block);

-- 完整性违规状态表（轻量追加，不修改主表）
-- 每次完整性校验若发现违规，向此表写入一条记录即可
-- 主表保持不变，查询时 LEFT JOIN 此表判断文件是否有效
CREATE TABLE file_integrity_violations
(
    file_path       String,    -- 关联 parquet_file_index.file_path
    detected_at     DateTime,  -- 违规发现时间
    expected_sha256 String,    -- 期望哈希（来自 parquet_file_index）
    actual_sha256   String,    -- 实际计算的哈希（或 "IO_ERROR" 等错误标记）
    violation_type  String     -- HASH_MISMATCH / IO_ERROR / FILE_MISSING
)
ENGINE = MergeTree()
ORDER BY (file_path, detected_at);
```

**回测引擎查询示例（排除已知违规文件）：**

```sql
-- 查询 Uniswap V3（ETH 主网），区块 19,000,000 到 19,100,000 的有效 Parquet 文件
SELECT idx.file_path, idx.start_eth_block, idx.end_eth_block, idx.data_sha256
FROM parquet_file_index AS idx
LEFT JOIN (
    -- 取每个文件的最新一次违规记录
    SELECT file_path, max(detected_at) AS latest_violation
    FROM file_integrity_violations
    GROUP BY file_path
) AS vio ON idx.file_path = vio.file_path
WHERE idx.source_chain_id = 1
  AND idx.protocol_id     = 'UNISWAP_V3_ETH'
  AND idx.start_eth_block <= 19100000
  AND idx.end_eth_block   >= 19000000
  AND vio.file_path       IS NULL   -- 排除有违规记录的文件
ORDER BY idx.start_eth_block;
```

---

## 6. PRD 数据精度要求（实现对照）

> PRD《数据基础设施规范》§4.1 定义了精度底线，本节说明 Archiver 如何满足每项要求。

| 数据类型 | PRD 最低精度要求 | 禁止使用 | 实现方式 |
|---|---|---|---|
| 价格数据 | 每笔 Swap 事件的精确成交价 | 1分钟/小时 OHLCV | Archiver 消费 `stream:chain:events` 中的每笔 Swap 事件，不做任何聚合，原样写入 Parquet |
| 流动性数据 | 每个 Mint/Burn 事件后的流动性快照 | 日均流动性 | Parquet Schema 中的 `liquidity_after` 字段，在每个 Mint/Burn 事件后由 L2 Data-Processor 采集注入 |
| Gas 价格 | 每个区块的 baseFee + priorityFee 记录 | 月均 Gas 均值 | `gas_price_wei` 字段记录每笔交易的 effective gas price；baseFee 单独通过 `event_type=BLOCK_GAS` 事件写入 |
| 链上延迟 | 每笔交易的 blockTimestamp - submitTimestamp | 固定延迟假设 | `anchor_timestamp_ms - ingest_timestamp_ms` 可计算摄取延迟；完整链上延迟需结合 FSM 的 `created_at` 与 `confirmed_at` 计算（见 §3.5）|

---

## 7. 数据完整性校验

### 7.1 每日区块哈希校验

每日凌晨（低峰期）自动运行完整性校验任务：

```python
# integrity_checker.py（示意）
async def daily_integrity_check(chain_id: int, date: datetime.date):
    """
    对前一天归档的全部 Parquet 文件做哈希校验，
    检测 S3 静默数据损坏或未授权篡改。
    """
    files = await clickhouse.query("""
        SELECT file_path, data_sha256, start_eth_block, end_eth_block
        FROM parquet_file_index
        WHERE source_chain_id = {chain_id}
          AND date = {date}
    """, chain_id=chain_id, date=date)

    for file in files:
        # 1. 从 S3 下载文件（流式，不全量加载内存）
        actual_hash = await compute_sha256_streaming(file.file_path)

        # 2. 对比 ClickHouse 中记录的期望哈希
        if actual_hash != file.data_sha256:
            # 触发 P0 告警：数据完整性被破坏
            await alert_p0(
                title   = "PARQUET_INTEGRITY_VIOLATION",
                details = f"File {file.file_path}: "
                          f"expected {file.data_sha256}, got {actual_hash}",
            )
            # 向违规状态表写入一条记录（主表保持不变，避免 ClickHouse Mutation）
            # 回测引擎查询时 LEFT JOIN file_integrity_violations 自动排除此文件（见 §5）
            await clickhouse.execute("""
                INSERT INTO file_integrity_violations
                    (file_path, detected_at, expected_sha256, actual_sha256, violation_type)
                VALUES ({path}, now(), {expected}, {actual}, 'HASH_MISMATCH')
            """, path=file.file_path,
                 expected=file.data_sha256,
                 actual=actual_hash)
```

### 7.2 写入时完整性保证

Archiver 在每次 `flush()` 时同步执行以下校验，确保写入过程的原子性：

```
写入流程（含完整性保证）：

1. 构建 Parquet 字节流（内存中）
2. 计算 SHA-256 哈希（写入前）
3. 上传至 S3（使用 S3 Multipart Upload，失败自动重试）
4. S3 返回 ETag（S3 内置 MD5）→ 与预期对比，确认传输无损
5. 写入 ClickHouse 索引（含 SHA-256 哈希）
6. 仅当步骤 3-5 全部成功后，才执行 Redis XACK

若步骤 3-5 任意一步失败：
- 不执行 XACK
- 缓冲区数据保留，下次 flush 重试
- 连续失败 3 次触发 P1 告警
```

---

## 8. 数据版本管理与可复现性

> PRD《数据基础设施规范》§4.3 定义了可复现性要求，本节说明实现机制。

每次回测任务必须能在未来任意时间点精确复现，Archiver 通过以下机制提供支撑：

| 可复现要素 | PRD 要求 | 实现机制 |
|---|---|---|
| **数据范围锁定** | 记录 start_block 和 end_block | 回测引擎从 ClickHouse 查询 `file_path` 列表时，同步记录 `start_eth_block` 和 `end_eth_block` 至回测报告 |
| **数据版本哈希** | 对数据集计算 SHA-256 | 回测报告记录所用全部 Parquet 文件的 `data_sha256` 列表；复现时对比哈希，不一致则告警 |
| **协议参数快照** | 记录手续费率、LTV 等参数版本 | Archiver 在每个协议升级事件（GovernanceEvent）触发时，写入协议参数快照至 `protocol_param_snapshots` 表 |
| **随机数种子** | Fuzzing 和蒙特卡洛种子写入报告 | 由回测引擎在任务创建时生成并记录，不在 Archiver 层处理（见 §7.1）|
| **SDK 版本锁定** | 记录 SDK 和 EVM 数学库版本 | 由回测引擎任务元数据记录，不在 Archiver 层处理（见 §7.1）|

### 8.1 协议参数快照表

```sql
-- 协议参数快照：记录协议升级前后的参数变化
CREATE TABLE protocol_param_snapshots (
    snapshot_id         UUID         PRIMARY KEY DEFAULT gen_random_uuid(),
    protocol_id         VARCHAR(100) NOT NULL,
    chain_id            INT          NOT NULL,
    anchor_eth_block    BIGINT       NOT NULL,   -- 参数生效的 ETH 锚点区块
    param_name          VARCHAR(100) NOT NULL,   -- 如 "fee_tier", "ltv_ratio"
    param_value         JSONB        NOT NULL,   -- 参数值（JSON 格式，支持复杂结构）
    governance_tx_hash  VARCHAR(66),             -- 触发此参数变化的治理交易哈希
    created_at          TIMESTAMPTZ  NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_pps_lookup
    ON protocol_param_snapshots(protocol_id, chain_id, anchor_eth_block DESC);
```

---

## 9. 冷热迁移策略

```
近 90 天（温数据）                  90 天以上（冷数据）
    │                                    │
本地 SSD / NAS                      S3 / MinIO
    │                                    │
高速随机访问（回测优先）              低成本长期存储（审计/合规）
    │                                    │
    └──── 自动迁移任务（每日凌晨）────────┘
          · 查询 ClickHouse：date < NOW() - 90天
          · 将文件从本地 SSD 移至 S3 冷存储层
          · 更新 ClickHouse 中的 file_path（本地路径 → S3 路径）
          · 本地文件删除（释放 SSD 空间）
```

**迁移对回测透明：** 回测引擎始终从 ClickHouse 查询 `file_path`，无论文件在本地 SSD 还是 S3，`IDataProvider` 的实现自动路由（见 §7.1）。迁移过程中引擎无需感知存储层变化。

---

## 10. 配置参数参考

本文档相关的系统可配置参数（完整列表含默认值见 README §0.6）：

| 配置键 | 说明 |
|---|---|
| `archiver_batch_size` | 每批写入 Parquet 的 Tick 数据量 |
| `archiver_flush_interval_sec` | 强制刷写间隔（防止数据积压） |
| `redis_stream_maxlen` | Redis Stream 保留的最大消息数（决定热数据窗口大小） |

---

*上一篇：[2.3 多链数据统一规范](./2.3_多链数据统一规范.md) | 下一篇：[3.1 策略 Agent 与水位线拦截](../03_核心执行引擎/3.1_策略Agent与水位线拦截.md)*
